{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92555109-66d3-4d31-bf7f-4591080f7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Part 1: Prepare dataset:\n",
    "# a.Uplode dataset:\n",
    "url = \"https://github.com/kristenauriemma/595Project/raw/main/USvideos.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "# b.Replace all \"0\" values:\n",
    "median_likes = data['likes'].median() \n",
    "data['likes'] = data['likes'].replace(0, median_likes) \n",
    "\n",
    "median_dislikes = data['dislikes'].median() \n",
    "data['dislikes'] = data['dislikes'].replace(0, median_dislikes) \n",
    "\n",
    "median_comment = data['comment_count'].median() \n",
    "data['comment_count'] = data['comment_count'].replace(0, median_comment)\n",
    "\n",
    "data['publish_time'] = pd.to_datetime(data['publish_time'])\n",
    "data['publish_year'] = data['publish_time'].dt.year\n",
    "data['publish_month'] = data['publish_time'].dt.month\n",
    "data['publish_day'] = data['publish_time'].dt.day\n",
    "\n",
    "data['trending_date'] = pd.to_datetime(data['trending_date'], format='%y.%d.%m')\n",
    "data['trending_year'] = data['trending_date'].dt.year\n",
    "data['trending_month'] = data['trending_date'].dt.month\n",
    "data['trending_day'] = data['trending_date'].dt.day\n",
    "\n",
    "\n",
    "# c.Split training and testing dataset:\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "#print('train set is ', train_data.shape[0])   #train set is  32759\n",
    "#print('test set is ', test_data.shape[0])    #test set is  8190\n",
    "\n",
    "\n",
    "# d.Using 3 feature as training dataset:\n",
    "x_train_1 = train_data[['likes', 'dislikes', 'comment_count']].values\n",
    "y_train = train_data['views'].values\n",
    "x_test_1 = test_data[['likes', 'dislikes', 'comment_count']].values\n",
    "y_test = test_data['views'].values\n",
    "\n",
    "# e.Using 6 columns (10 features) as training dataset:\n",
    "x_train_2 = train_data[['likes', \n",
    "                        'dislikes', \n",
    "                        'comment_count', \n",
    "                        'publish_year', 'publish_month', 'publish_day', \n",
    "                        'trending_year', 'trending_month', 'trending_day', \n",
    "                        'category_id']]\n",
    "column_names = x_train_2.columns\n",
    "print(column_names)\n",
    "x_train_2 = x_train_2.values\n",
    "\n",
    "x_test_2 = test_data[['likes', \n",
    "                      'dislikes', \n",
    "                      'comment_count', \n",
    "                      'publish_year', 'publish_month', 'publish_day', \n",
    "                      'trending_year', 'trending_month', 'trending_day',\n",
    "                      'category_id']].values\n",
    "\n",
    "# f.Using 5 columns (7 features) as training dataset:\n",
    "x_train_3 = train_data[['likes', \n",
    "                        'dislikes', \n",
    "                        'comment_count', \n",
    "                        'publish_year', 'publish_month', 'publish_day', \n",
    "                        'category_id']].values\n",
    "x_test_3 = test_data[['likes', \n",
    "                      'dislikes', \n",
    "                      'comment_count', \n",
    "                      'publish_year', 'publish_month', 'publish_day', \n",
    "                      'category_id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d16aa-cc63-4706-8fe9-34c107633088",
   "metadata": {},
   "source": [
    "For the above section, I processed the data by selecting relevant columns, filling missing values with the median, and deriving the year, month, and day values from the time column. I prepared three different test datasets to train the model:\n",
    "1. x_train_1 and x_test_1: These datasets include the most impactful columns: 'likes', 'dislikes', and 'comment_count'. This train set is simple and effective; however, some data points lack values for all three columns, which significantly impacts the results.\n",
    "2. x_train_2 and x_test_2: In addition to the previous three columns, this train set includes 'category_id', 'publish_time', and 'trending_date'. This dataset contains all relevant columns but requires substantial memory to train the model.\n",
    "3. x_train_3 and x_test_3: This is the final train set I used for the initial part of my model training. After running the model several times with the previous datasets, I realized that the 'trending_date' column introduced noise, reducing the prediction accuracy instead of providing useful information. Therefore, I removed the 'trending_date' column from this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "011c5d0e-1cec-4f18-809b-d0358c5ab2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part 2: Forest and error checking functions:\n",
    "# a. First forest(base forest):\n",
    "def forest_model_1 (x,y):\n",
    "    forest = RandomForestClassifier(n_estimators=50, max_leaf_nodes=100, max_depth = 10, random_state=42)\n",
    "    forest.fit(x, y)\n",
    "    print(\"finish training\")\n",
    "    return forest\n",
    "\n",
    "# b. MSE and r2:\n",
    "def mse_r2(a, b):\n",
    "    mse = mean_squared_error(a, b)\n",
    "    r2 = r2_score(a, b)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"R-squared (R²):\", r2)\n",
    "    return mse, r2\n",
    "\n",
    "# c. plot the result:\n",
    "def result_show(a_pred, a_test):\n",
    "    a_pred = np.array(a_pred)\n",
    "    a_test = np.array(a_test)\n",
    "    sample_size = 100\n",
    "    \n",
    "    indices = np.linspace(0, len(a_pred) - 1, sample_size, dtype=int)\n",
    "    a_pred_sampled = a_pred[indices]\n",
    "    a_test_sampled = a_test[indices]\n",
    "    \n",
    "    plt.plot(indices, a_pred_sampled, 'r-', label='Predicted', linewidth=1)\n",
    "    plt.plot(indices, a_test_sampled, 'b.', label='Actual', markersize=2)\n",
    "    \n",
    "    plt.legend() \n",
    "    plt.xlabel('Index')  \n",
    "    plt.ylabel('Values') \n",
    "    plt.title('Prediction vs Actual')  \n",
    "    \n",
    "    plt.savefig('plot.png')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Part 3.Test functions:\n",
    "# a. test with 3 feature trainset:\n",
    "forest_test1 = forest_model_1(x_train_1, y_train)\n",
    "y_pred1 = forest_test1.predict(x_test_1)\n",
    "result_show(y_pred1, y_test)\n",
    "mse_1, r2_1 = mse_r2(y_test, y_pred1)\n",
    "\n",
    "# b. test with 10 feature trainset:\n",
    "forest_test2 = forest_model_1(x_train_2, y_train)\n",
    "y_pred2 = forest_test2.predict(x_test_2)\n",
    "mse_2, r2_2 = mse_r2(y_test, y_pred2)\n",
    "result_show(y_pred2, y_test)\n",
    "\n",
    "# c. test with 7 feature trainset:\n",
    "forest_test3 = forest_model_1(x_train_3, y_train)\n",
    "y_pred3 = forest_test3.predict(x_test_3)\n",
    "mse_2, r2_2 = mse_r2(y_test, y_pred3)\n",
    "result_show(y_pred3, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1b23-a023-46e7-9605-eca48b9a3b11",
   "metadata": {},
   "source": [
    "In this section, I build the first version of the random forest model: forest_model_1. In this model, I randomly selected the hyperparameters and ran three datasets to obtain the initial results. Based on the results from trainset 1 and 2:\n",
    "1. trainset 1 Result: Mean Squared Error (MSE): 11159728343252.725; R-squared (R²): 0.7682357226586797\n",
    "2. trainset 2 Result: Mean Squared Error (MSE): 17127661628281.486; R-squared (R²): 0.6442941980549763\n",
    "\n",
    "I began to notice that trainset 2 introduced too much noisy data, which may not be necessary. This is when I decided to add dataset 3 (trainset 3 Result: Mean Squared Error (MSE): 15638183450545.225; R-squared (R²): 0.675227552600961). So far, the dataset with 3 features (x_train_1) seems to work the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609e4ba-6563-4f15-b0ff-52b176152ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Part 4. Based on forest_model_1, adjust the hyperparmeter:\n",
    "# a. Tune the model:\n",
    "def forest_model_2 (x,y):\n",
    "    forest = RandomForestClassifier(n_estimators=100, max_leaf_nodes=100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 10, random_state=42)\n",
    "    forest.fit(x, y)\n",
    "    print(\"finish training\")\n",
    "    return forest\n",
    "\n",
    "# b, Test result:\n",
    "# trainset 1:\n",
    "forest_test1 = forest_model_2(x_train_1, y_train)\n",
    "y_pred1 = forest_test1.predict(x_test_1)\n",
    "mse_1, r2_1 = mse_r2(y_test, y_pred1)\n",
    "result_show(y_pred1, y_test)\n",
    "\n",
    "# trainset 2:\n",
    "forest_test2 = forest_model_2(x_train_2, y_train)\n",
    "y_pred2 = forest_test2.predict(x_test_2)\n",
    "mse_2, r2_2 = mse_r2(y_test, y_pred2)\n",
    "result_show(y_pred2, y_test)\n",
    "\n",
    "# trainset 3:\n",
    "forest_test3 = forest_model_2(x_train_3, y_train)\n",
    "y_pred3 = forest_test3.predict(x_test_3)\n",
    "mse_2, r2_2 = mse_r2(y_test, y_pred3)\n",
    "result_show(y_pred3, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc0399-debb-41ac-bec0-8e2e29576c50",
   "metadata": {},
   "source": [
    "I tuned the previous model in 'forest_model_2'. In this section, I tried adding hyperparameters such as 'max_leaf_nodes', 'min_samples_split', and 'min_samples_leaf', as well as adjusting the number of trees in the random forest. Here are some of the hyperparameters that I chose to tune:\n",
    "1. Result-1: n_estimators=100, max_leaf_nodes=100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 10, random_state=42\n",
    "2. Result-2: n_estimators=100, max_leaf_nodes=100, max_depth = 20, random_state=42\n",
    "3. Result-3: n_estimators=150, max_leaf_nodes=150, max_depth = 20, random_state=42 (Too much memory needed to train a model with over 3 features)\n",
    "4. Result-4: n_estimators=50, max_leaf_nodes=50, max_depth = 15, random_state=42\n",
    "5. Result-5: n_estimators=100, max_leaf_nodes=100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 12, random_state=42\n",
    "6. Result-6: n_estimators=100, max_leaf_nodes=100, min_samples_split = 2, min_samples_leaf = 1, max_depth = 10, random_state=42\n",
    "7. Result-7: n_estimators=100, max_leaf_nodes=100, min_samples_split = 10, min_samples_leaf = 5, max_depth = 10, random_state=42\n",
    "8. Result-test3_1: n_estimators=150, max_leaf_nodes=100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 10, random_state=42\n",
    "9. Result-test3-2: n_estimators=80, max_leaf_nodes=100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 10, random_state=42\n",
    "10. Result-test3-3: n_estimators=100, max_leaf_nodes=100, min_samples_split = 7, min_samples_leaf = 2, max_depth = 10, random_state=42\n",
    "11. Result-test3-4: n_estimators=100, max_leaf_nodes=100, min_samples_split = 5, min_samples_leaf = 3, max_depth = 10, random_state=42\n",
    "12. Result-test3-5: n_estimators=100, max_leaf_nodes=90, min_samples_split = 5, min_samples_leaf = 2, max_depth = 10, random_state=42\n",
    "\n",
    "Part of the result from trainset 1 (x_train_1) is listed below\n",
    "1. Result-1: Mean Squared Error (MSE): 14101218767308.586; R-squared (R²): 0.7071471028044247\n",
    "2. Result-2: Mean Squared Error (MSE): 11280566205582.316; R-squared (R²): 0.7657261723383781\n",
    "3. Result-3: Mean Squared Error (MSE): 11677284747260.941; R-squared (R²): 0.7574871558236401\n",
    "4. Result-4: Mean Squared Error (MSE): 21704152830206.836; R-squared (R²): 0.5492500228251708\n",
    "5. Result-5: Mean Squared Error (MSE): 9129241914461.613; R-squared (R²): 0.8104046899798829\n",
    "6. Result-6: Mean Squared Error (MSE): 11363043631200.883; R-squared (R²): 0.7640132882647244\n",
    "7. Result-7: Mean Squared Error (MSE): 9780747834352.56; R-squared (R²): 0.7968742711325092\n",
    "\n",
    "Part of the result from trainset 2 (x_train_2) is listed below:\n",
    "1. Result-1: Mean Squared Error (MSE): 14268376239509.443; R-squared (R²): 0.703675590814596\n",
    "2. Result-2: Mean Squared Error (MSE): 20341760804703.42; R-squared (R²): 0.5775440631041426\n",
    "As I expected before, the 10 features bring too much noisy data to the model, which requires a huge amount of memory and training time, resulting in an unsatisfactory model\n",
    "\n",
    "Part of the result from trainset 3 (x_train_3) is listed below:\n",
    "1. Result-1: Mean Squared Error (MSE): 8570610683605.1875: R-squared (R²): 0.8220062952822209\n",
    "2. Result-2: Mean Squared Error (MSE): 11139439161662.44; R-squared (R²): 0.7686570866349777\n",
    "3. Result-4: Mean Squared Error (MSE): 16460499335906.984; R-squared (R²): 0.6581497670980205\n",
    "4. Result-5: Mean Squared Error (MSE): 11852180042836.658; R-squared (R²): 0.7538549453842157\n",
    "5. Result-6: Mean Squared Error (MSE): 15419108474788.736; R-squared (R²): 0.6797772828343558\n",
    "6. Result-7: Mean Squared Error (MSE): 10948417476177.715; R-squared (R²): 0.7726242085514942\n",
    "7. Result-test3-1: Mean Squared Error (MSE): 9114062405514.55; R-squared (R²): 0.8107199367146871\n",
    "8. Result-test3-2: Mean Squared Error (MSE): 9284269449688.404; R-squared (R²): 0.8071850914767019\n",
    "9. Result-test3-3: Mean Squared Error (MSE): 13786198584874.736; R-squared (R²): 0.7136894148288795\n",
    "10. Result-test3-4: Mean Squared Error (MSE): 10924210810998.6; R-squared (R²): 0.7731269304896562\n",
    "11. Result-test3-5: Mean Squared Error (MSE): 10159916237541.361; R-squared (R²): 0.7889997343828\n",
    "\n",
    "From the result, I can see that my model starts to overfit several times when the value of the hyperparameter goes too high. With different datasets, different hyperparameters work differently. Based on the result, the trainset 3, which contains 7 features, works best with the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f32a0-9cce-49ba-a6a8-aea8b08abb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part 5: Get deep learning of the dataset: \n",
    "# a. Get to know the importance of each feature:\n",
    "# a-1: function of feature important calculation:\n",
    "def feature_import(model, x_train):\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': x_train.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(feature_importances)\n",
    "\n",
    "# a-2: Calculate the importance of each features:\n",
    "forest_test2 = forest_model_2(x_train_2, y_train)\n",
    "x_train_2_df = pd.DataFrame(x_train_2, columns=column_names)\n",
    "feature_import(forest_test2, x_train_2_df)\n",
    "y_pred2 = forest_test2.predict(x_test_2)\n",
    "mse_2, r2_2 = mse_r2(y_test, y_pred2)\n",
    "result_show(y_pred2, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1b732-e869-4c60-b7eb-01d646726909",
   "metadata": {},
   "source": [
    "By calculating the feature_importances_, the result shows as follows:\n",
    "\n",
    "0. |          Feature    | Importance  |\n",
    "1. | 0           likes   |   0.197535  |\n",
    "2. | 2   comment_count   |   0.182441  |\n",
    "3. | 1        dislikes   |   0.156043  |\n",
    "4. | 8    trending_day   |   0.131839  |\n",
    "5. | 5     publish_day   |   0.092326  |\n",
    "6. | 9     category_id   |   0.086993  |\n",
    "7. | 4   publish_month   |   0.053832  |\n",
    "8. | 7  trending_month   |   0.044207  |\n",
    "9. | 3    publish_year   |   0.043077  |\n",
    "10. | 6   trending_year   |   0.011708  |\n",
    "\n",
    "I learned which features affect the model's prediction the most. For this particular dataset, if the columns 'likes', 'dislikes', and 'comment_count' contain 0, it specifically means that no data is available. Since only a small amount of data falls into this category, I removed these meaningless entries and retrained the model to evaluate whether it improves the performance of my best model. Based on these factors, I modified the training set accordingly below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c7a81c39-c3be-40b5-bd67-3f316c554b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before: 40949\n",
      "Number of rows after: 40003\n",
      "train set is  32002\n",
      "test set is  8001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# b. Modified dataset again:\n",
    "no_zero_data = pd.read_csv(url)\n",
    "\n",
    "num_rows = len(no_zero_data)\n",
    "print(f\"Number of rows before: {num_rows}\") # Number of rows before: 40949\n",
    "\n",
    "no_zero_data = no_zero_data[(no_zero_data[['likes', 'dislikes', 'comment_count']] != 0).all(axis=1)]\n",
    "num_rows = len(no_zero_data)\n",
    "print(f\"Number of rows after: {num_rows}\") # Number of rows after: 40003\n",
    "\n",
    "no_zero_data['publish_time'] = pd.to_datetime(no_zero_data['publish_time'])\n",
    "no_zero_data['publish_day'] = no_zero_data['publish_time'].dt.day\n",
    "\n",
    "no_zero_data['trending_date'] = pd.to_datetime(no_zero_data['trending_date'], format='%y.%d.%m')\n",
    "no_zero_data['trending_day'] = no_zero_data['trending_date'].dt.day\n",
    "\n",
    "no_zero_train, no_zero_test= train_test_split(no_zero_data, test_size=0.2, random_state=42)\n",
    "print('train set is ', no_zero_train.shape[0]) # train set is  32002\n",
    "print('test set is ', no_zero_test.shape[0]) # test set is  8001\n",
    "\n",
    "no_zero_trainX= no_zero_train[['likes', 'dislikes', 'comment_count', 'publish_day', 'trending_day', 'category_id']].values\n",
    "no_zero_trainY = no_zero_train['views'].values\n",
    "no_zero_testX = no_zero_test[['likes', 'dislikes', 'comment_count', 'publish_day', 'trending_day', 'category_id']].values\n",
    "no_zero_testY = no_zero_test['views'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c96a5-60c0-4c4e-a3f2-5e3ced0520fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# b-1: test new dataset with best model:\n",
    "print('start test:')\n",
    "no_zero_test = forest_model_2(no_zero_trainX, no_zero_trainY)\n",
    "no_zero_y = no_zero_test.predict(no_zero_testX)\n",
    "mse_1, r2_1 = mse_r2(no_zero_testY, no_zero_y)\n",
    "result_show(no_zero_y, no_zero_testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda9223-5e28-4692-a6d3-74f8344a7e94",
   "metadata": {},
   "source": [
    "Here is the result of no_zero dataset on the best model:\n",
    "1. Mean Squared Error (MSE): 13283701336064.434\n",
    "2. R-squared (R²): 0.8140916033865432\n",
    "\n",
    "From the result, the non zero dataset isn't really improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c37b1f-d704-4527-b3b3-33cd1dff4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# b-2: test with boostrap:\n",
    "print('start test:')\n",
    "no_zero_trainX_df = pd.DataFrame(no_zero_trainX)\n",
    "bootstrapped_x_train = no_zero_trainX_df.sample(n=len(no_zero_trainX_df), replace=True)\n",
    "bootstrap_test = forest_model_2(bootstrapped_x_train, no_zero_trainY)\n",
    "bootstrap_y = bootstrap_test.predict(no_zero_testX)\n",
    "mse_1, r2_1 = mse_r2(no_zero_testY, bootstrap_y )\n",
    "result_show(bootstrap_y , no_zero_testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb5ebd-03a3-43ed-825a-57dc5782c43c",
   "metadata": {},
   "source": [
    "Here is the result of the bootstrapped dataset on the best model:\n",
    "1. Mean Squared Error (MSE): 85638016138213.78;\n",
    "2. R-squared (R²): -0.19852335329074267\n",
    "\n",
    "![Plot Result](../plot.png)\n",
    "\n",
    "\n",
    "\n",
    "Oh, I got the first negative R-squared result. From the result, the bootstrapped dataset actually decreased the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d6ae1-8eaa-4b20-b4b9-a6a50b0a6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. refill the missing values with the mean: \n",
    "\n",
    "mean_data = pd.read_csv(url)\n",
    "\n",
    "columns_to_replace = ['likes', 'dislikes', 'comment_count']\n",
    "for column in columns_to_replace:\n",
    "    mean_data[column] = mean_data[column].replace(0, mean_data[column].mean())\n",
    "\n",
    "mean_data['publish_time'] = pd.to_datetime(mean_data['publish_time'])\n",
    "mean_data['publish_day'] = mean_data['publish_time'].dt.day\n",
    "\n",
    "mean_data['trending_date'] = pd.to_datetime(mean_data['trending_date'], format='%y.%d.%m')\n",
    "mean_data['trending_day'] = mean_data['trending_date'].dt.day\n",
    "\n",
    "mean_train, mean_test= train_test_split(mean_data, test_size=0.2, random_state=42)\n",
    "print('train set is ', mean_train.shape[0])\n",
    "print('test set is ', mean_test.shape[0])\n",
    "\n",
    "mean_trainX= mean_train[['likes', 'dislikes', 'comment_count', 'publish_day', 'trending_day', 'category_id']].values\n",
    "mean_trainY = mean_train['views'].values\n",
    "mean_testX = mean_test[['likes', 'dislikes', 'comment_count', 'publish_day', 'trending_day', 'category_id']].values\n",
    "mean_testY = mean_test['views'].values\n",
    "\n",
    "\n",
    "# c-1: test with this new mean dataset:\n",
    "print('test start:')\n",
    "mean_test = forest_model_2(mean_trainX, mean_trainY)\n",
    "mean_y = mean_test.predict(mean_testX)\n",
    "mse_1, r2_1 = mse_r2(mean_testY, mean_y)\n",
    "result_show(mean_y, mean_testY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bdb8d8-5f11-4d4a-a4f6-a0039c874cbf",
   "metadata": {},
   "source": [
    "In the last part, I used the mean instead of the median to fill all 0 values and trained the model with the best configuration. By observing the results as follows:\n",
    "1. Mean Squared Error (MSE): 17244960096747.096\n",
    "2. R-squared (R²): 0.6418581535616878\n",
    "\n",
    "\n",
    "![Plot Result](../plot.png)\n",
    "\n",
    "The modification to the dataset didn't significantly impact the prediction accuracy of the model. Tuning the hyperparameters seemed to be more helpful for improving the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
